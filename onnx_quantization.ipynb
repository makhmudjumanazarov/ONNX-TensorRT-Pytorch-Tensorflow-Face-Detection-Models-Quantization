{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from onnxruntime.quantization import quantize_dynamic, QuantType\n",
    "import onnx\n",
    "from onnx import numpy_helper\n",
    "import os "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_fp32 = '/home/airi/yolo/ONNX-TensorRT-Pytorch-Tensorflow-Face-Detection-Models-Quantization/models/yolov8l.onnx'\n",
    "model_quant = '/home/airi/yolo/ONNX-TensorRT-Pytorch-Tensorflow-Face-Detection-Models-Quantization/quant_models/quant_yolov8l.onnx'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_bytes(num):\n",
    "    \"\"\"\n",
    "    this function will convert bytes to MB.... GB... etc\n",
    "    \"\"\"\n",
    "    for x in ['bytes', 'KB', 'MB', 'GB', 'TB']:\n",
    "        if num < 1024.0:\n",
    "            return \"%3.1f %s\" % (num, x)\n",
    "        num /= 1024.0\n",
    "\n",
    "\n",
    "def file_size(file_path):\n",
    "    \"\"\"\n",
    "    this function will return the file size\n",
    "    \"\"\"\n",
    "    if os.path.isfile(file_path):\n",
    "        file_info = os.stat(file_path)\n",
    "        return convert_bytes(file_info.st_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "166.6 MB\n"
     ]
    }
   ],
   "source": [
    "# Lets check the file size of MS Paint exe \n",
    "# or you can use any file path\n",
    "file_path = r\"/home/airi/yolo/ONNX-TensorRT-Pytorch-Tensorflow-Face-Detection-Models-Quantization/models/yolov8l.onnx\"\n",
    "print(file_size(file_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42.0 MB\n"
     ]
    }
   ],
   "source": [
    "# Quantized model\n",
    "file_path = r\"/home/airi/yolo/ONNX-TensorRT-Pytorch-Tensorflow-Face-Detection-Models-Quantization/quant_models/quant_yolov8l.onnx\"\n",
    "print(file_size(file_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83.4 MB\n"
     ]
    }
   ],
   "source": [
    "# Model Float16 Conversion\n",
    "file_path = r\"/home/airi/yolo/ONNX-TensorRT-Pytorch-Tensorflow-Face-Detection-Models-Quantization/quant_models/model_fp16.onnx\"\n",
    "print(file_size(file_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layers: 207\n",
      "[[-0.00551987]]\n",
      "Type: float32\n"
     ]
    }
   ],
   "source": [
    "model = onnx.load(model_fp32)\n",
    "INTIALIZERS=model.graph.initializer\n",
    "Weight=[]\n",
    "for initializer in INTIALIZERS:\n",
    "    W= numpy_helper.to_array(initializer)\n",
    "    Weight.append(W)\n",
    "print(f\"Layers: {len(Weight)}\")\n",
    "print(Weight[0][0][0])\n",
    "print(f\"Type: {Weight[0][0][0][0][0].dtype}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dynamic quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantize_dynamic(model_fp32,  model_quant, weight_type=QuantType.QUInt8) #chnage QInt8 to QUInt8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import onnxruntime as ort\n",
    "ort_session = ort.InferenceSession(model_quant, providers=['CPUExecutionProvider'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layers: 567\n",
      "2.9414062\n",
      "Type: float32\n"
     ]
    }
   ],
   "source": [
    "model  = onnx.load(model_quant)\n",
    "INTIALIZERS=model.graph.initializer\n",
    "Weight=[]\n",
    "for initializer in INTIALIZERS:\n",
    "    W= numpy_helper.to_array(initializer)\n",
    "    Weight.append(W)\n",
    "print(f\"Layers: {len(Weight)}\")\n",
    "print(Weight[0][0])\n",
    "print(f\"Type: {Weight[0][0].dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Float16 Conversion\n",
    "   \n",
    "   Converting a model to use float16 instead of float32 can decrease the model size (up to half) and improve performance \n",
    "on some GPUs. There may be some accuracy loss, but in many models the new accuracy is acceptable. Tuning data is not needed for float16 conversion, which can make it preferable to quantization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/airi/miniconda3/envs/mahmud/lib/python3.9/site-packages/onnxconverter_common/float16.py:43: UserWarning: the float32 number 7.522191936004674e-08 will be truncated to 1e-07\n",
      "  warnings.warn(\"the float32 number {} will be truncated to {}\".format(pos_min, min_positive_val))\n",
      "/home/airi/miniconda3/envs/mahmud/lib/python3.9/site-packages/onnxconverter_common/float16.py:53: UserWarning: the float32 number -6.257487683569707e-08 will be truncated to -1e-07\n",
      "  warnings.warn(\"the float32 number {} will be truncated to {}\".format(neg_max, -min_positive_val))\n",
      "/home/airi/miniconda3/envs/mahmud/lib/python3.9/site-packages/onnxconverter_common/float16.py:43: UserWarning: the float32 number 9.744358919760998e-08 will be truncated to 1e-07\n",
      "  warnings.warn(\"the float32 number {} will be truncated to {}\".format(pos_min, min_positive_val))\n",
      "/home/airi/miniconda3/envs/mahmud/lib/python3.9/site-packages/onnxconverter_common/float16.py:53: UserWarning: the float32 number -9.744358919760998e-08 will be truncated to -1e-07\n",
      "  warnings.warn(\"the float32 number {} will be truncated to {}\".format(neg_max, -min_positive_val))\n"
     ]
    }
   ],
   "source": [
    "from onnxconverter_common import float16\n",
    "\n",
    "model_fp32 = '/home/airi/yolo/ONNX-TensorRT-Pytorch-Tensorflow-Face-Detection-Models-Quantization/models/yolov8l.onnx'\n",
    "model = onnx.load(model_fp32)\n",
    "model_fp16 = float16.convert_float_to_float16(model)\n",
    "onnx.save(model_fp16, \"/home/airi/yolo/ONNX-TensorRT-Pytorch-Tensorflow-Face-Detection-Models-Quantization/quant_models/model_fp16.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layers: 207\n",
      "[[-0.00552]]\n",
      "Type: float16\n"
     ]
    }
   ],
   "source": [
    "model  = onnx.load(\"/home/airi/yolo/ONNX-TensorRT-Pytorch-Tensorflow-Face-Detection-Models-Quantization/quant_models/model_fp16.onnx\")\n",
    "INTIALIZERS=model.graph.initializer\n",
    "Weight=[]\n",
    "for initializer in INTIALIZERS:\n",
    "    W= numpy_helper.to_array(initializer)\n",
    "    Weight.append(W)\n",
    "print(f\"Layers: {len(Weight)}\")\n",
    "print(Weight[0][0][0])\n",
    "print(f\"Type: {Weight[0][0].dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mahmud",
   "language": "python",
   "name": "mahmud"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
